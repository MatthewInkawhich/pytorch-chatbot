{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying Seq2Seq Model with the Hybrid Frontend\n",
    "\n",
    "This tutorial will walk through the process of transitioning a sequence-to-sequence model to graph mode using PyTorch's Hybrid Frontend. The model that we will convert is the chatbot model from the [Chatbot tutorial](). While the [introductory Hybrid Frontend tutorials]() are useful for gaining an understanding of the work-flow, purpose, and basic syntax of the feature, this document covers a more challenging model and a more practical use-case. You can either treat this tutorial as a \"Part 2\" to the [Chatbot tutorial]() and deploy your own pretrained model, or you can start with this document and use a pretrained model that we host. In the latter case, you can reference the original [Chatbot tutorial]() for details regarding data preprocessing, model theory and definition, and model training.\n",
    "\n",
    "\n",
    "## What is the Hybrid Frontend?\n",
    "During the research and development phase of a deep learning-based project, it is adventagous to interact with an **eager**, imperative interface like PyTorch's. This gives users the ability to write familiar idiomatic Python that executes as-is, allowing for the use of Python data structures, control flow operations, print statements, and debugging utilities. Although the eager interface is an advantageous tool for R&D applications, when it comes time to deploy the model in a production environment, having a **graph**-based model representation is very beneficial. Having a deferred representation allows for numerous optimization techniques such as out-of-order execution, framework-agnostic model exportation with ONNX, and the ability to target highly optimized hardware architectures. The Hybrid Frontend provides a flexible and non-intrusive tool for translating models from eager mode to graph mode.\n",
    "\n",
    "The Hybrid Frontend is faciliated through a Just-In-Time (JIT) compiler (`torch.jit`). The JIT compiler has two core modalities for converting an eager model to a graph representation: **trace** and **script**. The `torch.jit.trace` function takes a module or function and an example input. It then runs the example input through the function or module while tracing the computational steps that are encountered, and outputs a graph-based function that performs the same actions. The **trace** mode is great for straightforward modules and functions that do not involve data-dependent control flow. However, if a function with data-dependent if statements and loops is traced, only the operations called using the control sequence taken by the example input will be recorded. In other words, the control flow itself is not captured. To convert modules and functions containing data-dependent control flows, a **script** mode is provided. The script mode explicitly converts the module or function code to graph mode, including all possible control flow routes. To use script mode, simply add a `torch.jit.script` decorator to your Python function or a `torch.jit.script_method` decorator to your module's `forward` function. The one caveat with using script mode is that it currently only supports a restricted subset of Python. As of now, features such as generators, defs, and Python data structures are not supported. To remedy this, you can invoke traced modules from script modules (and vice-versa), and you can call pure Python functions and modules from script modules. However, the operations done in the pure Python functions will not be compiled, and will run as-is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Environment\n",
    "\n",
    "First, we will import the required modules and set some constants. If you are planning on using your own model, be sure that the `MAX_LENGTH` constant is set correctly. As a reminder, this constant defines the maximum allowed sentence length during training and the maximum length output that the model is capable of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import torch\n",
    "from torch.jit import script, trace\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "\n",
    "# Constants\n",
    "MAX_LENGTH = 10  # Maximum sentence length\n",
    "\n",
    "# Default word tokens\n",
    "PAD_token = 0  # Used for padding short sentences\n",
    "SOS_token = 1  # Start-of-sentence token\n",
    "EOS_token = 2  # End-of-sentence token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Overview\n",
    "\n",
    "As mentioned, the model that we are using is a [sequence-to-sequence](https://arxiv.org/abs/1409.3215) (seq2seq) model. This type of model is used in cases when our input is a variable-length sequence, and our output is also a variable length sequence that is not necessarily a one-to-one mapping of the input. A seq2seq model is comprised of two recurrent neural networks (RNNs) that work cooperatively: an **encoder** and a **decoder**.\n",
    "\n",
    "![model](images/seq2seq_model.png)\n",
    "\n",
    "Image source: https://medium.com/botsupply/generative-model-chatbots-e422ab08461e\n",
    "\n",
    "### Encoder\n",
    "\n",
    "The encoder RNN iterates through the input sentence one word at a time, at each time step outputting an \"output\" vector and a hidden state that is used in the next time step. Essentially, the encoder takes the input sequence, and attempts to encode its \"meaning\" to a fixed-sized context or \"thought\" tensor. In our case, the context tensor is simply the output of the RNN, or the output features of the last hidden layer in the network.\n",
    "\n",
    "\n",
    "### Decoder\n",
    "\n",
    "The decoder RNN generates the response sentence in a word-by-word fashion. It uses the encoder's context tensor, and a previous word to generate the next word in the sequence. It continues generating words until it outputs an *EOS_token*, representing the end of the sentence. In practice, relying soley on the encoder's context tensor to encode the meaning of the entire sequence is not very effective, especially when encoding long input sequences. To remedy this, we use an [attention mechanism](https://arxiv.org/abs/1409.0473) in our decoder. This attention mechanism helps the decoder to \"pay attention\" to certain parts of the input when generating the output. For our model, we implement [Luong et al.](https://arxiv.org/abs/1508.04025)'s \"Global attention\" module, and use it as a layer in our decode model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Encoder\n",
    "\n",
    "We implement our encoder's RNN with the `torch.nn.GRU` module which we feed an entire batch of sentences (word embeddings) and it internally iterates through the sequences one word at a time calculating the hidden states. We initialize this module to be bidirectional, meaning that we have two independent GRUs: one that iterates through the senquences in chronological order, and another that iterates in reverse order. We ultimately return the sum of these models' outputs. Since our model was trained using batching, our `EncoderRNN` model's `forward` function expects a padded input batch. To batch variable-length sentences, we allow a maximum of *MAX_LENGTH* words in a sentence, and all sentences in the batch that have less than *MAX_LENGTH* words are padded at the end with our dedicated *PAD_token* tokens. To use padded batches with one of PyTorch's RNN modules, we must wrap the GRU forward pass call with `torch.nn.utils.rnn.pack_padded_sequence` and `torch.nn.utils.rnn.pad_packed_sequence` data transformations. Note that the `forward` function also takes an `input_lengths` list, which contains the length of each sentence in the batch. This input is used by the `torch.nn.utils.rnn.pack_padded_sequence` function when padding.\n",
    "\n",
    "#### Hybrid Frontend\n",
    "\n",
    "Since the encoder's `forward` function does not contain any data-dependent control flow, we will use the **trace** method for converting it to graph mode. When tracing a module, we can leave the module definition as-is, as we perform the trace upon initialization. We will initialize all models towards the end of this document before we run evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "        \n",
    "        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n",
    "        #   because our input size is a word embedding with number of features = hidden_size\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        # Convert word indexes to embeddings\n",
    "        embedded = self.embedding(input_seq)\n",
    "        # Pack padded batch of sequences for RNN module\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        # Forward pass through GRU\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        # Unpack padding\n",
    "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # Sum bidirectional GRU outputs\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Decoder's Attention Module\n",
    "\n",
    "#### Hybrid Frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Luong attention layer\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "\n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        max_len = encoder_outputs.size(0)\n",
    "        batch_size = encoder_outputs.size(1)\n",
    "\n",
    "        # Create variable to store attention energies\n",
    "        attn_energies = torch.zeros(batch_size, max_len) # B x S\n",
    "        attn_energies = attn_energies.to(device)\n",
    "\n",
    "        # For each batch of encoder outputs\n",
    "        for b in range(batch_size):\n",
    "            # Calculate energy for each encoder output\n",
    "            for i in range(max_len):\n",
    "                attn_energies[b, i] = self.score(hidden[:, b], encoder_outputs[i, b].unsqueeze(0))\n",
    "\n",
    "        # Normalize energies to weights in range 0 to 1, resize to 1 x B x S\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)\n",
    "\n",
    "    # Score functions\n",
    "    def score(self, hidden, encoder_output):\n",
    "        if self.method == 'dot':\n",
    "            energy = hidden.squeeze(0).dot(encoder_output.squeeze(0))\n",
    "            return energy\n",
    "\n",
    "        elif self.method == 'general':\n",
    "            energy = self.attn(encoder_output)\n",
    "            energy = hidden.squeeze(0).dot(energy.squeeze(0))\n",
    "            return energy\n",
    "\n",
    "        elif self.method == 'concat':\n",
    "            energy = self.attn(torch.cat((hidden, encoder_output), 1))\n",
    "            energy = self.v.squeeze(0).dot(energy.squeeze(0))\n",
    "            return energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Decoder\n",
    "\n",
    "Similarly to the `EncoderRNN`, we use the `torch.nn.GRU` module for our decoder's RNN. This time, however, we use a unidirectional GRU. It is important to note that unlike the encoder, we will feed the decoder RNN one word at a time. We start by getting the embedding of the current word and applying a dropout. Next, we forward the embedding and the last hidden state to the GRU and obtain a current GRU output and hidden state. We then use our attention module as a layer to obtain the attention weights, which we multiply by the encoder's output to obtain our attended encoder output. The attended encoder output represents a weighted sum indicating what parts of the encoder's output to pay attention to. From here, we use a linear layer and softmax normalization to select the next word in the output sequence.\n",
    "\n",
    "#### Hybrid Frontend\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(torch.jit.ScriptModule):\n",
    "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    @torch.jit.script_method\n",
    "    def forward(self, input_seq, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step (word) at a time\n",
    "        # Get embedding of current input word\n",
    "        embedded = self.embedding(input_seq)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        # Forward through unidirectional GRU\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "        # Calculate attention weights from the current GRU output\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        # Concatenate weighted context vector and GRU output using Luong eq. 5\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1) \n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        # Predict next word using Luong eq. 6\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Handling\n",
    "\n",
    "Although our models conceptually deal with sequences of words, in reality, they deal with numbers like all machine learning models do. In this case, every word in the model's vocabulary, which was established before training, is mapped to an integer index. We use a `Voc` object to contain the mappings from word to index, as well as the total number of words in the vocabulary. We will load the object later before we run the model.\n",
    " \n",
    "Also, in order for us to be able to run evaluations, we must provide a tool for processing our string inputs. The `normalizeString` function converts all characters in a string to lowercase and removes all non-letter characters. The `indexesFromSentence` function takes a sentence of words and returns the corresponding sequence of word indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Voc:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3  # Count SOS, EOS, PAD\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    # Remove words below a certain count threshold\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "\n",
    "        keep_words = []\n",
    "\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "\n",
    "        print('keep_words {} / {} = {:.4f}'.format(\n",
    "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
    "        ))\n",
    "\n",
    "        # Reinitialize dictionaries\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3 # Count default tokens\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.addWord(word)\n",
    "            \n",
    "\n",
    "# Lowercase and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def indexesFromSentence(voc, sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Evaluation\n",
    "\n",
    "### Decoding\n",
    "\n",
    "To decode a given decoder output, we must iteratively run forward passes through our `decoder` model, which outputs softmax scores corresponding to the probability of each word being the correct next word in the decoded sequence. We initialize the `decoder_input` to a tensor containing an *SOS_token*. After each pass through the `decoder`, we append the word with the highest softmax probability to the `decoded_words` list. We also use this word as the `decoder_input` for the next iteration. The decoding process terminates either if the `decoded_words` list has reached a length of *MAX_LENGTH* or if the predicted word is the *EOS_token*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(decoder, decoder_hidden, encoder_outputs, voc, max_length=MAX_LENGTH):\n",
    "    # Initialize input, words, and attentions\n",
    "    decoder_input = torch.LongTensor([[SOS_token]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "    decoded_words = []\n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "    \n",
    "    # Allow output sequences with a max length of max_length\n",
    "    for _ in range(max_length):\n",
    "        # Run forward pass though decoder model\n",
    "        decoder_output, decoder_hidden = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs\n",
    "        )\n",
    "        \n",
    "        # Take word with highest softmax probability\n",
    "        _, topi = decoder_output.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        # If the recommended word is an EOS token, append the token to the decoded_words list and stop decoding\n",
    "        if ni == EOS_token:\n",
    "            break\n",
    "        # Else, append the string word to decoded_words list\n",
    "        else:\n",
    "            decoded_words.append(voc.index2word[ni.item()])\n",
    "\n",
    "        # Set next decoder input as the chosen decoded word\n",
    "        decoder_input = torch.LongTensor([[ni]])\n",
    "        decoder_input = decoder_input.to(device)\n",
    "\n",
    "    return decoded_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating an Input\n",
    "\n",
    "Next, we define an `evaluate` function to drive an input through our seq2seq model. This function takes a normalized string `sentence`, prepares it in a batch of size 1. This batch is then passed through the encoder along with a `lengths` tensor. We then prepare the encoder's final hidden layer to be the first hidden input to the decoder by simply slicing out an appropriate amount of layers. Finally, we call our `decode` function with the `decoder_hidden` and `encoder_outputs` tensors that we created, and return the output.\n",
    "\n",
    "We also define an `evaluateInput` function, which takes a string sentence, normalizes it, calls the evaluate function, and prints the results in a conversational format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate a sentence\n",
    "def evaluate(encoder, decoder, voc, sentence, max_length=MAX_LENGTH):\n",
    "    # Format input sentence as a batch\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
    "    lengths = [len(indexes) for indexes in indexes_batch]\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    input_batch = input_batch.to(device)\n",
    "\n",
    "    # Forward input through encoder model\n",
    "    encoder_outputs, encoder_hidden = encoder(input_batch, torch.tensor(lengths))\n",
    "\n",
    "    # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "\n",
    "    # Decode sentence\n",
    "    return decode(decoder, decoder_hidden, encoder_outputs, voc)\n",
    "\n",
    "\n",
    "# Normalize input sentence and call evaluate()\n",
    "def evaluateExample(sentence, encoder, decoder, voc):\n",
    "    print(\"> \" + sentence)\n",
    "    # Normalize sentence\n",
    "    input_sentence = normalizeString(sentence)\n",
    "    # Evaluate sentence\n",
    "    output_words = evaluate(encoder, decoder, voc, input_sentence)\n",
    "    output_sentence = ' '.join(output_words)\n",
    "    print('bot:', output_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Parameters\n",
    "\n",
    "Ok, its time to load our model!\n",
    "\n",
    "### Use hosted model\n",
    "\n",
    "### Use my own model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n"
     ]
    }
   ],
   "source": [
    "save_dir = os.path.join(\"data\", \"save\")\n",
    "corpus_name = \"cornell movie-dialogs corpus\"\n",
    "\n",
    "# Configure models\n",
    "model_name = 'model7'\n",
    "attn_model = 'dot'\n",
    "#attn_model = 'general'\n",
    "#attn_model = 'concat'\n",
    "hidden_size = 500\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "# Set checkpoint to load from; set to None if starting from scratch\n",
    "checkpoint_iter = 4000\n",
    "loadFilename = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size), '{}_checkpoint.tar'.format(checkpoint_iter))\n",
    "\n",
    "\n",
    "#checkpoint = torch.load(loadFilename)\n",
    "checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "encoder_sd = checkpoint['en']\n",
    "decoder_sd = checkpoint['de']\n",
    "embedding_sd = checkpoint['embedding']\n",
    "voc = checkpoint['voc']\n",
    "\n",
    "# Initialize Model\n",
    "checkpoint = None\n",
    "print('Building encoder and decoder ...')\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "embedding.load_state_dict(embedding_sd)\n",
    "\n",
    "# Initialize encoder\n",
    "dummy_seq = torch.zeros((1,1), dtype=torch.int64)\n",
    "dummy_lengths = torch.tensor([1])\n",
    "encoder = trace(dummy_seq, dummy_lengths)(EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout))\n",
    "\n",
    "# Initialize decoder\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
    "\n",
    "# Populate model state_dicts\n",
    "encoder.load_state_dict(encoder_sd)\n",
    "decoder.load_state_dict(decoder_sd)\n",
    "\n",
    "\n",
    "# use cuda\n",
    "#encoder = encoder.to(device)\n",
    "#decoder = decoder.to(device)\n",
    "print('Models built and ready to go!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder graph graph(%0 : Long(1, 1)\n",
      "      %1 : Long(1)\n",
      "      %2 : Float(7826, 500)\n",
      "      %3 : Float(1500, 500)\n",
      "      %4 : Float(1500, 500)\n",
      "      %5 : Float(1500)\n",
      "      %6 : Float(1500)\n",
      "      %7 : Float(1500, 500)\n",
      "      %8 : Float(1500, 500)\n",
      "      %9 : Float(1500)\n",
      "      %10 : Float(1500)\n",
      "      %11 : Float(1500, 1000)\n",
      "      %12 : Float(1500, 500)\n",
      "      %13 : Float(1500)\n",
      "      %14 : Float(1500)\n",
      "      %15 : Float(1500, 1000)\n",
      "      %16 : Float(1500, 500)\n",
      "      %17 : Float(1500)\n",
      "      %18 : Float(1500)) {\n",
      "  %19 : int = prim::Constant[value=-1](), scope: EncoderRNN/Embedding[embedding]\n",
      "  %20 : int = prim::Constant[value=0](), scope: EncoderRNN/Embedding[embedding]\n",
      "  %21 : int = prim::Constant[value=0](), scope: EncoderRNN/Embedding[embedding]\n",
      "  %22 : Float(1, 1, 500) = aten::embedding(%2, %0, %19, %20, %21), scope: EncoderRNN/Embedding[embedding]\n",
      "  %70 : Float(1, 500), %71 : Long(1) = ^pack_padded_sequence_trace_wrapper()(%22, %1), scope: EncoderRNN\n",
      "  %83 : Float(4, 1, 500) = prim::Constant[value=<Tensor>](), scope: EncoderRNN/GRU[gru]\n",
      "  %501 : Float(1, 1000), %502 : Float(4, 1, 500) = ^forward_flattened_wrapper()(%70, %3, %4, %5, %6, %7, %8, %9, %10, %11, %12, %13, %14, %15, %16, %17, %18, %83, %71), scope: EncoderRNN/GRU[gru]\n",
      "  %549 : Float(1, 1, 1000), %550 : Long(1) = ^pad_packed_sequence_trace_wrapper()(%501, %71), scope: EncoderRNN\n",
      "  %551 : int = prim::Constant[value=2](), scope: EncoderRNN\n",
      "  %552 : int = prim::Constant[value=0](), scope: EncoderRNN\n",
      "  %553 : int = prim::Constant[value=500](), scope: EncoderRNN\n",
      "  %554 : int = prim::Constant[value=1](), scope: EncoderRNN\n",
      "  %555 : Float(1, 1!, 500) = aten::slice(%549, %551, %552, %553, %554), scope: EncoderRNN\n",
      "  %556 : int = prim::Constant[value=2](), scope: EncoderRNN\n",
      "  %557 : int = prim::Constant[value=500](), scope: EncoderRNN\n",
      "  %558 : int = prim::Constant[value=1000](), scope: EncoderRNN\n",
      "  %559 : int = prim::Constant[value=1](), scope: EncoderRNN\n",
      "  %560 : Float(1, 1!, 500) = aten::slice(%549, %556, %557, %558, %559), scope: EncoderRNN\n",
      "  %561 : int = prim::Constant[value=1](), scope: EncoderRNN\n",
      "  %562 : Float(1, 1!, 500) = aten::add(%555, %560, %561), scope: EncoderRNN\n",
      "  return (%562, %502);\n",
      "}\n",
      "\n",
      "decoder graph graph(%input_seq : Dynamic\n",
      "      %last_hidden : Dynamic\n",
      "      %encoder_outputs : Dynamic) {\n",
      "  %embedded.1 : Dynamic = ^<python_value>()(%input_seq)\n",
      "  %embedded : Dynamic = ^<python_value>()(%embedded.1)\n",
      "  %rnn_output.1 : Dynamic, %hidden : Dynamic = ^<python_value>()(%embedded, %last_hidden)\n",
      "  %attn_weights : Dynamic = ^<python_value>()(%rnn_output.1, %encoder_outputs)\n",
      "  %9 : int = prim::Constant[value=0]()\n",
      "  %10 : int = prim::Constant[value=1]()\n",
      "  %11 : Dynamic = aten::transpose(%encoder_outputs, %9, %10)\n",
      "  %context.1 : Dynamic = aten::bmm(%attn_weights, %11)\n",
      "  %13 : int = prim::Constant[value=0]()\n",
      "  %rnn_output : Dynamic = aten::squeeze(%rnn_output.1, %13)\n",
      "  %15 : int = prim::Constant[value=1]()\n",
      "  %context : Dynamic = aten::squeeze(%context.1, %15)\n",
      "  %18 : int = prim::Constant[value=1]()\n",
      "  %19 : Dynamic[] = prim::ListConstruct(%rnn_output, %context)\n",
      "  %concat_input : Dynamic = aten::cat(%19, %18)\n",
      "  %21 : Dynamic = ^<python_value>()(%concat_input)\n",
      "  %concat_output : Dynamic = aten::tanh(%21)\n",
      "  %output.1 : Dynamic = ^<python_value>()(%concat_output)\n",
      "  %24 : int = prim::Constant[value=1]()\n",
      "  %output : Dynamic = aten::softmax(%output.1, %24)\n",
      "  return (%output, %hidden);\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('encoder graph', encoder.__getattr__('forward').graph)\n",
    "print('decoder graph', decoder.__getattr__('forward').graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> hello\n",
      "bot: hello .\n",
      "> what's up?\n",
      "bot: i m sorry .\n",
      "> who are you?\n",
      "bot: no one s in the trunk .\n",
      "> where are we?\n",
      "bot: we re in the garage .\n",
      "> where are you from?\n",
      "bot: the zoo .\n"
     ]
    }
   ],
   "source": [
    "# Set dropout layers to eval mode\n",
    "#encoder.eval()\n",
    "#decoder.eval()\n",
    "\n",
    "# Evaluate examples\n",
    "sentences = [\"hello\", \"what's up?\", \"who are you?\", \"where are we?\", \"where are you from?\"]\n",
    "for s in sentences:\n",
    "    evaluateExample(s, encoder, decoder, voc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pc36]",
   "language": "python",
   "name": "conda-env-pc36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
